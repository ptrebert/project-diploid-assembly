{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FTP FASTQ libs:  2304\n",
      "OK libs  1603\n",
      "Blacklist  701\n",
      "100c controls:  24\n",
      "-- in AS list:  []\n",
      "wgs cells:  124\n",
      "-- in AS list:  []\n",
      "Final blacklist:  701\n",
      "HG02492 :  42\n",
      "GM18939 :  41\n",
      "HG00864 :  40\n",
      "HG02587 :  37\n",
      "HG02011 :  36\n",
      "HG03732 :  34\n",
      "HG01114 :  34\n",
      "HG01573 :  34\n",
      "GM20509B :  33\n",
      "HG03065 :  31\n",
      "HG02018 :  30\n",
      "GM19983 :  30\n",
      "HG03371 :  30\n",
      "HG00171A :  29\n",
      "GM12329 :  28\n",
      "HG01505 :  27\n",
      "HG01596 :  26\n",
      "GM20847B :  24\n",
      "HG03009 :  23\n",
      "GM19036B :  22\n",
      "HG03683 :  20\n",
      "GM19650A :  19\n",
      "GM18534B :  17\n",
      "HG00096 :  14\n",
      "Sanity check\n",
      "Blacklisted \"but\" annotated:  122\n",
      "Thereof, dropped/low qual:  122\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import os\n",
    "import ftplib\n",
    "import collections as col\n",
    "\n",
    "def keep_item(item):\n",
    "    if 'manifest' in item.lower():\n",
    "        return False\n",
    "    if 'readme' in item.lower():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def traverse_remote_path(ftp_server, remote_url):\n",
    "    remote_listing = ftp_server.nlst(remote_url)\n",
    "    remote_files = []\n",
    "\n",
    "    for item in remote_listing:\n",
    "        # simplistic heuristic: if it has a file extension, it's a file\n",
    "        # works for intended use case\n",
    "        if len(item.split('.')) > 1:\n",
    "            # it's a file\n",
    "            remote_files.append(item)\n",
    "            continue\n",
    "        try:\n",
    "            sub_dir_listing = ftp_server.nlst(item)\n",
    "        except Exception as error:\n",
    "            continue\n",
    "        remote_files.extend(sub_dir_listing)\n",
    "\n",
    "    remote_files = [os.path.basename(f).split('_')[0] for f in remote_files]\n",
    "    remote_files = set(filter(keep_item, remote_files))\n",
    "\n",
    "    return sorted(remote_files)\n",
    "\n",
    "cache_folder = '/home/peter/work/code/github/ptrebert/project-diploid-assembly/annotation'\n",
    "cache_file = 'cache_ftp_sseq_fastq.txt'\n",
    "store_file = os.path.join(cache_folder, cache_file)\n",
    "\n",
    "if not os.path.isfile(store_file):\n",
    "    data_source = 'vol1/ftp/data_collections/HGSVC2/working/20200120_Strandseq/fastq'\n",
    "    server_url = 'ftp.1000genomes.ebi.ac.uk'\n",
    "    server = ftplib.FTP(server_url)\n",
    "    server.login()\n",
    "\n",
    "    sseq_fastq = traverse_remote_path(server, data_source)\n",
    "    print('Writing cache file')\n",
    "    with open(store_file, 'w') as dump:\n",
    "        _ = dump.write('\\n'.join(sseq_fastq))\n",
    "\n",
    "with open(store_file, 'r') as dump:\n",
    "    sseq_libs = set(dump.read().strip().split())\n",
    "\n",
    "print('FTP FASTQ libs: ', len(sseq_libs))\n",
    "\n",
    "cells_per_sample = col.Counter([l.split('x')[0] for l in sseq_libs])\n",
    "# for HGSVC2, there should be 96 cells per sample\n",
    "assert all([x == 96 for x in cells_per_sample.values()]), 'Cells per sample missing: {}'.format(count_libs)\n",
    "\n",
    "selection = '../annotation/20200128_ASanders_QCselect_HGSVClibs.txt'\n",
    "controls = '../annotation/20200507_ASanders_100cell_controls.txt'\n",
    "wgs = '../annotation/20200507_ASanders_wgs_cells.txt'\n",
    "\n",
    "df = pandas.read_csv(selection, sep='\\t', comment='#')\n",
    "\n",
    "# Ashley's annotation is more restrictive as it targets\n",
    "# inversion analysis; for the clustering, it's just important\n",
    "# to get rid of the complete garbage\n",
    "high_qual = df.loc[df['score'] == 1, :]\n",
    "ok_qual = df.loc[((df['score'] == 0) & (df['reads'] > 50000)), :]\n",
    "low_qual = df.loc[((df['score'] == 0) & (df['reads'] <= 50000)), 'cell']\n",
    "\n",
    "accepted_libs = set(high_qual['cell'].values).union(set(ok_qual['cell'].values))\n",
    "print('OK libs ', len(accepted_libs))\n",
    "\n",
    "blacklist_from_ftp = sseq_libs.difference(accepted_libs)\n",
    "print('Blacklist ', len(blacklist_from_ftp))\n",
    "\n",
    "with open(controls, 'r') as annotation:\n",
    "    control_cells = set(annotation.read().strip().split())\n",
    "print('100c controls: ', len(control_cells))\n",
    "    \n",
    "accept_controls = []\n",
    "for c in control_cells:\n",
    "    if c in blacklist_from_ftp:\n",
    "        continue\n",
    "    elif c in accepted_libs:\n",
    "        accept_controls.append(c)\n",
    "    else:\n",
    "        print('-- Unknown: {}'.format(c))\n",
    "print('-- in AS list: ', accept_controls)\n",
    "\n",
    "with open(wgs, 'r') as annotation:\n",
    "    wgs_cells = set(annotation.read().strip().split())\n",
    "print('wgs cells: ', len(wgs_cells))\n",
    "    \n",
    "accept_wgs = []\n",
    "for w in wgs_cells:\n",
    "    if w in blacklist_from_ftp:\n",
    "        continue\n",
    "    elif w in accepted_libs:\n",
    "        accept_wgs.append(w)\n",
    "    else:\n",
    "        print('-- Unknown: {}'.format(w))\n",
    "print('-- in AS list: ', accept_wgs)\n",
    "\n",
    "final_blacklist = blacklist_from_ftp.union(control_cells, wgs_cells)\n",
    "print('Final blacklist: ', len(final_blacklist))\n",
    "\n",
    "cells_blacklisted = col.Counter([c.split('x')[0] for c in final_blacklist])\n",
    "for sample, num_blacklisted in cells_blacklisted.most_common():\n",
    "    print(sample, ': ', num_blacklisted)\n",
    "assert not any([n >= 96 for n in cells_blacklisted.values()]), \\\n",
    "    'All cells blacklisted: {}'.format(\n",
    "        [(sample, n) for (sample, n) in cells_blacklisted.most_common() if n >= 96]\n",
    "    )\n",
    "\n",
    "\n",
    "# The overlap between Ashley's annotation and the blacklisted\n",
    "# libraries should be non-zero (e.g., because of the \"0\" quality\n",
    "# libs with less than 50000 reads). On the other hand, it should\n",
    "# also not be too large because the complete garbage is not part\n",
    "# of Ashley's annotation.\n",
    "\n",
    "print('Sanity check')\n",
    "sanity_check = final_blacklist.intersection(set(df['cell'].values))\n",
    "print('Blacklisted \"but\" annotated: ', len(sanity_check))\n",
    "print('Thereof, dropped/low qual: ', low_qual.shape[0])\n",
    "\n",
    "blacklist = '../annotation/hgsvc_blacklist.txt'\n",
    "\n",
    "with open(blacklist, 'w') as dump:\n",
    "    _ = dump.write('\\n'.join(sorted(final_blacklist)))\n",
    "\n",
    "# update: fixed Strand-seq blacklist\n",
    "# after double-checking with A. Sanders\n",
    "# important fixes:\n",
    "# - correct NA20509 blacklist now\n",
    "# - library HG03009x02PE20386: from WGS to \"good/1\"\n",
    "# MD5: b75aa05368ca490a36aafcf44a00b161\n",
    "\n",
    "# update: Strand-seq data for NA20509 added\n",
    "# blacklist generated on 2020-05-13\n",
    "# MD5: ee7ccc87c35c8359b37da8e3f5559124\n",
    "    \n",
    "# blacklist generated on 2020-05-07\n",
    "# MD5: cc6f92724d97034523b68adadec1c9fa\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
