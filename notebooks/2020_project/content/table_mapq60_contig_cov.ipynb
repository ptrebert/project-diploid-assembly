{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    region_set  region_count\n",
      "0              wg_noGiemPosVar           286\n",
      "1        wg_noGiemPosVar_noGap           823\n",
      "2   wg_noGiemPosVar_noGap_122X           508\n",
      "3  wg_noGiemPosVar_noGap_122XY           561\n",
      "4                 whole_genome           194\n",
      "whole_genome\n",
      "3099750718\n",
      "wg_noGiemPosVar\n",
      "2467823303\n",
      "wg_noGiemPosVar_noGap\n",
      "2432321108\n",
      "wg_noGiemPosVar_noGap_122XY\n",
      "2421332465\n",
      "wg_noGiemPosVar_noGap_122X\n",
      "2395484832\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import collections as col\n",
    "\n",
    "\"\"\"\n",
    "What does this do?\n",
    "Dump Supp. Table \"MAPQ60 contig coverage\" relative to various subsets of the\n",
    "full GRCh38 reference assembly. Requires output of \"recomp_cov.py\" script (line ~35ff)\n",
    "\"\"\"\n",
    "\n",
    "path2 = '/home/local/work/data/hgsvc/aln_summary/ps_ctgref_alnstore.h5'\n",
    "\n",
    "males = [\n",
    "    'HG02011',\n",
    "    'HG03371',\n",
    "    'HG03065',\n",
    "    'NA19239',\n",
    "    'NA19650',\n",
    "    'HG00731',\n",
    "    'NA18534',\n",
    "    'HG00512',\n",
    "    'HG01596',\n",
    "    'NA24385',\n",
    "    'HG00096',\n",
    "    'HG01505',\n",
    "    'NA20509',\n",
    "    'HG03009',\n",
    "    'HG03732',\n",
    "    'HG02492'\n",
    "]\n",
    "\n",
    "tsv_path = '/home/local/work/data/hgsvc/aln_summary/regions'\n",
    "cache_file = '/home/local/work/data/hgsvc/aln_summary/ctg_region_aln.cache.h5'\n",
    "\n",
    "region_desc = {\n",
    "    'tsv': 'whole_genome',\n",
    "    'giemsa.tsv': 'wg_noGiemPosVar',\n",
    "    'giemsa.nogap.tsv': 'wg_noGiemPosVar_noGap',\n",
    "    'giemsa.nogap.122XY.tsv': 'wg_noGiemPosVar_noGap_122XY',\n",
    "    'giemsa.nogap.122X.tsv': 'wg_noGiemPosVar_noGap_122X',\n",
    "}\n",
    "\n",
    "table_columns = [\n",
    "    'chrom',\n",
    "    'start',\n",
    "    'end',\n",
    "    'num_overlaps',\n",
    "    'coverage_bp',\n",
    "    'length',\n",
    "    'coverage_frac'\n",
    "]\n",
    "\n",
    "def extract_sample_info(file_path):\n",
    "    file_name = os.path.basename(file_path)\n",
    "    sample = file_name.split('_')[0]\n",
    "    if sample not in males:\n",
    "        sex = 'F'\n",
    "    else:\n",
    "        sex = 'M'\n",
    "    if 'clr' in file_name:\n",
    "        platform = 'CLR'\n",
    "    else:\n",
    "        platform = 'HiFi'\n",
    "    if 'h1-un' in file_name:\n",
    "        hap = 'H1'\n",
    "    else:\n",
    "        hap = 'H2'\n",
    "    region_id = region_desc[file_name.split('cov-in_GRCh38_HGSVC2_noalt.')[-1]]\n",
    "    return sample, sex, platform, hap, region_id\n",
    "\n",
    "\n",
    "def cache_region_coverage():\n",
    "    \n",
    "    region_length = set()\n",
    "    region_count = set()\n",
    "    \n",
    "    temp_store = col.defaultdict(list)\n",
    "        \n",
    "    for table in os.listdir(tsv_path):\n",
    "        if not table.endswith('.tsv'):\n",
    "            continue\n",
    "        table_path = os.path.join(tsv_path, table)\n",
    "        sample, sex, platform, hap, region_set = extract_sample_info(table)\n",
    "        aligns = pd.read_csv(\n",
    "            table_path,\n",
    "            sep='\\t',\n",
    "            header=None,\n",
    "            names=table_columns\n",
    "        )\n",
    "        region_count.add((region_set, aligns.shape[0]))\n",
    "        region_length.update(set((region_set, t[0], t[1], t[5]) for t in aligns.itertuples(index=False, name=None)))\n",
    "        \n",
    "        row_index = pd.MultiIndex.from_tuples(\n",
    "            [(region_set, t[0], t[1]) for t in aligns.itertuples(index=False, name=None)],\n",
    "            names=['region_set', 'chrom', 'start']\n",
    "        )\n",
    "        col_index = pd.MultiIndex.from_tuples(\n",
    "            [(sample, sex, platform, hap)],\n",
    "            names=['sample', 'sex', 'platform', 'hap']\n",
    "        )\n",
    "        \n",
    "        aligns.index = row_index\n",
    "        aligns.drop([\n",
    "            'chrom',\n",
    "            'start',\n",
    "            'end',\n",
    "            'num_overlaps',\n",
    "            'length',\n",
    "            'coverage_frac'],\n",
    "            axis=1,\n",
    "            inplace=True\n",
    "        )\n",
    "        aligns.columns = col_index\n",
    "        temp_store[region_set].append(aligns)\n",
    "        \n",
    "    row_concat = []\n",
    "    for dataframes in temp_store.values():\n",
    "        tmp = pd.concat(dataframes, axis=1, ignore_index=False)\n",
    "        row_concat.append(tmp)\n",
    "        \n",
    "    final = pd.concat(row_concat, axis=0, ignore_index=False)\n",
    "\n",
    "    metadata_count = pd.DataFrame(\n",
    "        sorted(region_count),\n",
    "        columns=['region_set', 'region_count']\n",
    "    )\n",
    "    \n",
    "    metadata_length = pd.DataFrame(\n",
    "        sorted(region_length),\n",
    "        columns=['region_set', 'chrom', 'start', 'length']\n",
    "    )\n",
    "    metadata_length.index = pd.MultiIndex.from_tuples(\n",
    "        metadata_length[['region_set', 'chrom', 'start']].itertuples(index=False, name=None),\n",
    "        names=['region_set', 'chrom', 'start']\n",
    "    )\n",
    "    metadata_length.drop(['region_set', 'chrom', 'start'], axis=1, inplace=True)\n",
    "    \n",
    "    with pd.HDFStore(cache_file, 'w', complevel=5) as hdf:\n",
    "        hdf.put('cache', final, format='fixed')\n",
    "        hdf.put('region_length', metadata_length, format='fixed')\n",
    "        hdf.put('region_count', metadata_count, format='fixed')\n",
    "    return\n",
    "        \n",
    "        \n",
    "if not os.path.isfile(cache_file):\n",
    "    cache_region_coverage()\n",
    "\n",
    "with pd.HDFStore(cache_file, 'r') as hdf:\n",
    "    data = hdf['cache']\n",
    "    lengths = hdf['region_length']\n",
    "    counts = hdf['region_count']\n",
    "print(counts)\n",
    "row_indices = []\n",
    "rows = []\n",
    "    \n",
    "for reg in region_desc.values():\n",
    "    print(reg)\n",
    "    region_cov = data.xs(reg, level='region_set').sum(axis=0)\n",
    "    rows.append(region_cov)\n",
    "    row_indices.append((reg, 'bp_covered'))\n",
    "    \n",
    "    genome_length = int(lengths.xs(reg, level='region_set').sum(axis=0))\n",
    "    print(genome_length)\n",
    "    region_cov_pct = ((region_cov / genome_length) * 100).round(2)\n",
    "    rows.append(region_cov_pct)\n",
    "    row_indices.append((reg, 'pct_covered'))\n",
    "    \n",
    "summary = pd.DataFrame(\n",
    "    rows,\n",
    "    index=pd.MultiIndex.from_tuples(\n",
    "        row_indices,\n",
    "        names=['region_set', 'statistic']\n",
    "    )\n",
    ")\n",
    "\n",
    "summary = summary.transpose()\n",
    "\n",
    "dump_tsv = '/home/local/work/data/hgsvc/aln_summary/genome_coverage.tsv'\n",
    "\n",
    "summary.sort_index(level=['platform', 'sample', 'hap'], inplace=True)\n",
    "\n",
    "summary.to_csv(\n",
    "    dump_tsv,\n",
    "    sep='\\t',\n",
    "    header=True,\n",
    "    index=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
