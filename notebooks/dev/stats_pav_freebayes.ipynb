{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('DEL', 'CLR', 'low_cov') :  19\n",
      "('DEL', 'CLR', 'missing') :  567\n",
      "('DEL', 'CLR', 'partial') :  792\n",
      "('DEL', 'HiFi', 'low_cov') :  17\n",
      "('DEL', 'HiFi', 'missing') :  239\n",
      "('DEL', 'HiFi', 'partial') :  524\n",
      "('DEL', 'low_cov') :  36\n",
      "('DEL', 'missing') :  806\n",
      "('DEL', 'partial') :  1316\n",
      "('INS', 'CLR', 'missing') :  731\n",
      "('INS', 'CLR', 'partial') :  1145\n",
      "('INS', 'HiFi', 'missing') :  267\n",
      "('INS', 'HiFi', 'partial') :  363\n",
      "('INS', 'missing') :  998\n",
      "('INS', 'partial') :  1508\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os as os\n",
    "import collections as col\n",
    "\n",
    "print_PAV_freebayes = False\n",
    "print_PAV_hapcov = False\n",
    "\n",
    "freebayes_calls = '/home/local/work/data/hgsvc/assembly_sv/freebayes_indels.h5'\n",
    "\n",
    "select_from_annotation = [\n",
    "    'ID',\n",
    "    'DISC_CLASS',\n",
    "    'SVLEN',\n",
    "    'SVTYPE',\n",
    "    'HAP',\n",
    "    'POS',\n",
    "    'END',\n",
    "    'MERGE_SAMPLES'\n",
    "]\n",
    "\n",
    "pav_annotation = '/home/local/work/data/hgsvc/assembly_sv/PAV_sv-insdel_v3.h5'\n",
    "def load_discovery_class(hdfstore=pav_annotation):\n",
    "    with pd.HDFStore(hdfstore, 'r') as hdf:\n",
    "        df = hdf['PAV_v3']\n",
    "        dropped_sv = df.loc[df['quality'] == '0', select_from_annotation].copy()\n",
    "        dropped_sv['sample'] = dropped_sv['MERGE_SAMPLES'].str.extract('(?P<sample>^[A-Z0-9]+)', expand=True)\n",
    "        dropped_sv.sort_values('ID', inplace=True)\n",
    "        dropped_sv.index = dropped_sv['ID']\n",
    "        dropped_sv.drop('ID', axis=1, inplace=True)\n",
    "    return dropped_sv\n",
    "\n",
    "\n",
    "def extract_row_index(fname, basic=False):\n",
    "    \n",
    "    sample = fname.split('_', 1)[0]\n",
    "    if 'pbsq2-ccs' in fname:\n",
    "        platform = 'HiFi'\n",
    "    elif 'pbsq2-clr' in fname:\n",
    "        platform = 'CLR'\n",
    "    else:\n",
    "        raise\n",
    "    if 'h1-un' in fname:\n",
    "        hap = 10\n",
    "    elif 'h2-un' in fname:\n",
    "        hap = 20\n",
    "    else:\n",
    "        raise\n",
    "    d = {\n",
    "        'sample': sample,\n",
    "        'platform': platform,\n",
    "        'hap': hap\n",
    "    }\n",
    "    if not basic:\n",
    "        if 'ins' in fname:\n",
    "            svtype = 'INS'\n",
    "        elif 'dels' in fname:\n",
    "            svtype = 'DEL'\n",
    "        else:\n",
    "            raise\n",
    "        if 'hom' in fname:\n",
    "            genotype = 'HOM'\n",
    "        elif 'het' in fname:\n",
    "            genotype = 'HET'\n",
    "        else:\n",
    "            raise\n",
    "        if 'out-hc' in fname:\n",
    "            loc = 'low_conf'\n",
    "        elif 'in-hc' in fname:\n",
    "            loc = 'high_conf'\n",
    "        else:\n",
    "            raise\n",
    "        d = {\n",
    "            'sample': sample,\n",
    "            'platform': platform,\n",
    "            'hap': hap,\n",
    "            'var_type': svtype,\n",
    "            'genotype': genotype,\n",
    "            'location': loc\n",
    "        }\n",
    "    return d\n",
    "    \n",
    "\n",
    "def parse_pav_overlaps(fpath):\n",
    "    \n",
    "    df = pd.read_csv(\n",
    "        fpath,\n",
    "        names=['ID', 'contig', 'overlap'],\n",
    "        usecols=[3, 7, 10],\n",
    "        sep='\\t',\n",
    "        header=None\n",
    "    )\n",
    "    df = df.loc[df['overlap'] > 0, :].copy()\n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_pav_summary(bed_path, cache_path):\n",
    "    dropped_sv = load_discovery_class()\n",
    "\n",
    "    columns = []\n",
    "    with pd.HDFStore(freebayes_calls, 'r') as hdf:\n",
    "        for bed_file in os.listdir(bed_path):\n",
    "            h = extract_row_index(bed_file)\n",
    "            fb_calls_key = os.path.join(h['sample'], h['platform'], 'HAP' + str(h['hap']))\n",
    "            fb_calls = hdf[fb_calls_key]\n",
    "            fb_calls = fb_calls.xs([h['var_type'], h['genotype']], level=['var_type', 'genotype'])\n",
    "            fb_calls['contig'] = fb_calls['sequence'] + '_' + \\\n",
    "                                fb_calls['start'].astype(str) + '_' + \\\n",
    "                                fb_calls['stop'].astype(str)\n",
    "            overlaps = parse_pav_overlaps(os.path.join(path, bed_file))\n",
    "            if overlaps.shape[0] == 0:\n",
    "                continue\n",
    "            for idx, row in overlaps.iterrows():\n",
    "                sv_data = dropped_sv.loc[row.ID]\n",
    "                if sv_data['SVTYPE'] != h['var_type']:\n",
    "                    continue\n",
    "                if h['sample'] not in sv_data['MERGE_SAMPLES']:\n",
    "                    sample_match = 'miss'\n",
    "                else:\n",
    "                    sample_match = 'match'\n",
    "                src_var_len = int(fb_calls.loc[fb_calls['contig'] == row['contig'], 'variant_length'])\n",
    "                pav_var_len = int(sv_data['SVLEN'])\n",
    "                if src_var_len != pav_var_len:\n",
    "                    length_match = 'partial'\n",
    "                elif abs(src_var_len - pav_var_len) == 1 and src_var_len > 2:\n",
    "                    raise ValueError('{} - {}'.format(bed_file, row.ID))\n",
    "                else:\n",
    "                    length_match = 'full'\n",
    "                if int(row.overlap) == pav_var_len:\n",
    "                    ovl_match = 'full'\n",
    "                else:\n",
    "                    ovl_match = 'partial'\n",
    "                record = dict(h)\n",
    "                record['ID'] = row.ID\n",
    "                record['sample_match'] = sample_match\n",
    "                record['var_len_match'] = length_match\n",
    "                record['ovl_len_match'] = ovl_match\n",
    "                record['overlap'] = row.overlap\n",
    "                record['fb_var_len'] = src_var_len\n",
    "                record['pav_var_len'] = sv_data['SVLEN']\n",
    "                record['pav_disc_class'] = sv_data['DISC_CLASS']\n",
    "                columns.append(record)\n",
    "    df = pd.DataFrame.from_records(columns)\n",
    "    df.to_hdf(cache_path, 'PAV_v3_fb', mode='w', format='fixed', complevel=9)\n",
    "    return None\n",
    "\n",
    "\n",
    "def prepare_hapcov_summary(tab_path, cache_path):\n",
    "    dropped_sv = load_discovery_class()\n",
    "\n",
    "    frames = []\n",
    "    for tab_file in os.listdir(tab_path):\n",
    "        if not (tab_file.endswith('h1-un.tab') or tab_file.endswith('h2-un.tab')):\n",
    "            continue\n",
    "        header = extract_row_index(tab_file.split('_AVG_')[1], basic=True)\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(tab_path, tab_file),\n",
    "            sep='\\t',\n",
    "            names=['ID', 'length', 'covered', 'cov_dp'],\n",
    "            usecols=[0, 1, 2, 4]\n",
    "        )\n",
    "        df.index = df['ID']\n",
    "        df = pd.concat([df, dropped_sv[['SVTYPE', 'sample']]], axis=1, ignore_index=False)\n",
    "        df['cov_bp_pct'] = ((df['covered'] / df['length']) * 100).round(2)\n",
    "        df.drop(['covered', 'length'], axis=1, inplace=True)\n",
    "        \n",
    "        df.index = pd.MultiIndex.from_arrays(\n",
    "            [df['ID'], df['SVTYPE'], df['sample']],\n",
    "            names=['ID', 'SVTYPE', 'sample'])\n",
    "        df.drop(['ID', 'SVTYPE', 'sample'], axis=1, inplace=True)\n",
    "        df.columns = pd.MultiIndex.from_tuples([\n",
    "            (header['sample'], header['platform'], header['hap'], 'depth'),\n",
    "            (header['sample'], header['platform'], header['hap'], 'ratio'),\n",
    "        ], names=['sample', 'platform', 'hap', 'cov'])\n",
    "        frames.append(df)\n",
    "    \n",
    "    frames = pd.concat(frames, axis=1, ignore_index=False)\n",
    "    frames.to_hdf(cache_path, 'PAV_v3_cov', mode='w', format='fixed', complevel=9)\n",
    "    return None\n",
    "\n",
    "\n",
    "def prepare_hapassm_summary(tsv_path, cache_path):\n",
    "    dropped_sv = load_discovery_class()\n",
    "\n",
    "    frames = []\n",
    "    for tsv_file in os.listdir(tsv_path):\n",
    "        if not tsv_file.endswith('.tsv'):\n",
    "            continue\n",
    "        header = extract_row_index(tsv_file.split('_OVL_')[1], basic=True)\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(tsv_path, tsv_file),\n",
    "            sep='\\t',\n",
    "            names=['ID', 'overlap'],\n",
    "            usecols=[3, 7],\n",
    "            dtype={'ID': str, 'overlap': int}\n",
    "        )\n",
    "        num_aln = df['ID'].value_counts()\n",
    "        bp_aln = df.groupby('ID')['overlap'].sum()\n",
    "        df.drop_duplicates('ID', inplace=True)\n",
    "        df.index = df['ID']\n",
    "        df['num_aln'] = num_aln\n",
    "        df['bp_aln'] = bp_aln\n",
    "        df.loc[df['bp_aln'] == 0, 'num_aln'] = 0\n",
    "        df.drop('overlap', axis=1, inplace=True)\n",
    "        \n",
    "        df = pd.concat([df, dropped_sv[['SVTYPE', 'SVLEN', 'sample']] ], axis=1, ignore_index=False)\n",
    "        df['SVLEN'] = df['SVLEN'].astype('int64')\n",
    "\n",
    "        df['aln_bp_pct'] = 0\n",
    "        select_ins = df['SVTYPE'] == 'INS'\n",
    "        select_ovl = df['bp_aln'] > 0\n",
    "        df.loc[(select_ins & select_ovl), 'aln_bp_pct'] = 1\n",
    "        select_del = df['SVTYPE'] == 'DEL'\n",
    "        \n",
    "        df.loc[select_del, 'aln_bp_pct'] = df.loc[select_del, 'bp_aln'] / df.loc[select_del, 'SVLEN']\n",
    "        df['aln_bp_pct'] = (df['aln_bp_pct'] * 100).round(2)\n",
    "                \n",
    "        df.index = pd.MultiIndex.from_arrays(\n",
    "            [df['ID'], df['SVTYPE'], df['sample']],\n",
    "            names=['ID', 'SVTYPE', 'sample'])\n",
    "        df.drop(['ID', 'SVTYPE', 'sample', 'SVLEN'], axis=1, inplace=True)\n",
    "        \n",
    "        df.columns = pd.MultiIndex.from_tuples([\n",
    "            (header['sample'], header['platform'], header['hap'], 'num_aln'),\n",
    "            (header['sample'], header['platform'], header['hap'], 'bp_aln'),\n",
    "            (header['sample'], header['platform'], header['hap'], 'ratio'),\n",
    "        ], names=['sample', 'platform', 'hap', 'cov'])\n",
    "        frames.append(df)\n",
    "    \n",
    "    frames = pd.concat(frames, axis=1, ignore_index=False)\n",
    "    frames.to_hdf(cache_path, 'PAV_v3_assm', mode='w', format='fixed', complevel=9)\n",
    "    return None\n",
    "\n",
    "\n",
    "if print_PAV_freebayes:\n",
    "    path = '/home/local/work/data/hgsvc/assembly_sv/65-PAV-GRCh38_HGSVC2_noalt'\n",
    "    cache_path = '/home/local/work/data/hgsvc/assembly_sv/cache.PAV_sv-insdel_v3.freebayes.h5'\n",
    "    if not os.path.isfile(cache_path):\n",
    "        prepare_pav_summary(path, cache_path)\n",
    "    df = pd.read_hdf(cache_path, 'PAV_v3_fb')\n",
    "    print(df.shape)\n",
    "    print(df['ID'].unique().shape)\n",
    "\n",
    "    df = df.loc[df['fb_var_len'] > 1, :].copy()\n",
    "    print(df.shape)\n",
    "    print(df['ID'].unique().shape)\n",
    "\n",
    "    print(df['pav_disc_class'].value_counts())\n",
    "    print(df['sample_match'].value_counts())\n",
    "    print(df['var_len_match'].value_counts())\n",
    "    print(df['location'].value_counts())\n",
    "    print(df['ovl_len_match'].value_counts())\n",
    "\n",
    "if print_PAV_hapcov:\n",
    "    path = '/home/local/work/data/hgsvc/assembly_sv/hap_read_coverage'\n",
    "    cache_path = '/home/local/work/data/hgsvc/assembly_sv/cache.PAV_sv-insdel_v3.hapcov.h5'\n",
    "    if not os.path.isfile(cache_path):\n",
    "        prepare_hapcov_summary(path, cache_path)\n",
    "    df = pd.read_hdf(cache_path, 'PAV_v3_cov')\n",
    "    stats = col.Counter()\n",
    "    for s in df.index.get_level_values('sample').unique():\n",
    "        rows = df.xs(s, level='sample', axis=0)\n",
    "        try:\n",
    "            cols = rows.xs(s, level='sample', axis=1)\n",
    "        except KeyError:\n",
    "            print('Skipping sample {}'.format(s))\n",
    "            continue\n",
    "        var_ins = cols.xs('INS', level='SVTYPE').xs('depth', level='cov', axis=1)\n",
    "        var_del = cols.xs('DEL', level='SVTYPE').xs('ratio', level='cov', axis=1)\n",
    "        for p, t in [('HiFi', 10), ('CLR', 30)]:\n",
    "            try:\n",
    "                tmp = var_ins.xs(p, level='platform', axis=1)\n",
    "                stats[('INS', p, 'low_cov')] += (tmp < t).any(axis=1).sum()\n",
    "            except KeyError:\n",
    "                pass\n",
    "            try:\n",
    "                tmp = var_del.xs(p, level='platform', axis=1)\n",
    "                stats[('DEL', p, 'low_cov')] += (tmp < 50).any(axis=1).sum()\n",
    "            except KeyError:\n",
    "                pass\n",
    "    print(stats)\n",
    "\n",
    "if True:\n",
    "    path = '/home/local/work/data/hgsvc/assembly_sv/hap_assm_coverage'\n",
    "    cache_path = '/home/local/work/data/hgsvc/assembly_sv/cache.PAV_sv-insdel_v3.hapassm.h5'\n",
    "    if not os.path.isfile(cache_path):\n",
    "        prepare_hapassm_summary(path, cache_path)\n",
    "    df = pd.read_hdf(cache_path, 'PAV_v3_assm')\n",
    "    stats = col.Counter()\n",
    "    for s in df.index.get_level_values('sample').unique():\n",
    "        rows = df.xs(s, level='sample', axis=0)\n",
    "        try:\n",
    "            cols = rows.xs(s, level='sample', axis=1)\n",
    "        except KeyError:\n",
    "            print('Skipping sample {}'.format(s))\n",
    "            continue\n",
    "        var_ins = cols.xs('INS', level='SVTYPE').xs('ratio', level='cov', axis=1)\n",
    "        for p in ['HiFi', 'CLR']:\n",
    "            try:\n",
    "                tmp = var_ins.xs(p, level='platform', axis=1)\n",
    "            except KeyError:\n",
    "                continue\n",
    "            missing = (tmp == 0).all(axis=1).sum()\n",
    "            partial = (tmp == 0).any(axis=1).sum()\n",
    "            partial -= missing\n",
    "            stats[('INS', p, 'missing')] += missing\n",
    "            stats[('INS', 'missing')] += missing\n",
    "            stats[('INS', p, 'partial')] += partial\n",
    "            stats[('INS', 'partial')] += partial\n",
    "\n",
    "        var_del = cols.xs('DEL', level='SVTYPE').xs('ratio', level='cov', axis=1)\n",
    "        for p in ['HiFi', 'CLR']:\n",
    "            try:\n",
    "                tmp = var_del.xs(p, level='platform', axis=1)\n",
    "            except KeyError:\n",
    "                continue\n",
    "            missing = (tmp == 0).all(axis=1).sum()\n",
    "            partial = (tmp == 0).any(axis=1).sum()\n",
    "            partial -= missing\n",
    "            low_cov = (tmp < 50).all(axis=1).sum()\n",
    "            low_cov -= missing\n",
    "            stats[('DEL', p, 'missing')] += missing\n",
    "            stats[('DEL', 'missing')] += missing\n",
    "            stats[('DEL', p, 'partial')] += partial\n",
    "            stats[('DEL', 'partial')] += partial\n",
    "            stats[('DEL', p, 'low_cov')] += low_cov\n",
    "            stats[('DEL', 'low_cov')] += low_cov\n",
    "    for k in sorted(stats.keys()):\n",
    "        print(k, ': ', stats[k])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
