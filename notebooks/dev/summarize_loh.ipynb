{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clr mean  2412559.975609756\n",
      "hifi mean  1857356.5129533678\n",
      "Counter({('HG00513', 'HiFi'): 35, ('NA12878', 'HiFi'): 35, ('HG00512', 'HiFi'): 25, ('HG00731', 'HiFi'): 25, ('HG00732', 'HiFi'): 24, ('NA24385', 'HiFi'): 15, ('NA19238', 'HiFi'): 9, ('NA19650', 'CLR'): 9, ('HG00171', 'CLR'): 8, ('HG00732', 'CLR'): 8, ('HG03486', 'HiFi'): 8, ('NA19239', 'HiFi'): 8, ('HG00096', 'CLR'): 7, ('HG01114', 'CLR'): 7, ('NA20847', 'CLR'): 7, ('HG02818', 'HiFi'): 5, ('HG03125', 'HiFi'): 4, ('HG00864', 'CLR'): 3, ('HG01505', 'CLR'): 3, ('HG03065', 'CLR'): 3, ('NA12329', 'CLR'): 3, ('HG00731', 'CLR'): 2, ('HG02011', 'CLR'): 2, ('HG03371', 'CLR'): 2, ('HG03683', 'CLR'): 2, ('HG03732', 'CLR'): 2, ('NA18534', 'CLR'): 2, ('NA20509', 'CLR'): 2, ('HG00512', 'CLR'): 1, ('HG00513', 'CLR'): 1, ('HG01596', 'CLR'): 1, ('HG02492', 'CLR'): 1, ('HG02587', 'CLR'): 1, ('HG03009', 'CLR'): 1, ('NA18939', 'CLR'): 1, ('NA19238', 'CLR'): 1, ('NA19239', 'CLR'): 1, ('NA19983', 'CLR'): 1})\n",
      "Unique LOH regions  261\n",
      "Unique LOH regions with DEL  19\n",
      "Regions identified  927\n",
      "15.0\n",
      "2.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-0fdc75eb0a89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhifi_loh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclr_loh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import collections as col\n",
    "import numpy as np\n",
    "import sklearn.cluster as sklclust\n",
    "import pickle as pck\n",
    "\n",
    "in_file = '/home/local/work/data/hgsvc/roi/20201026_DP_LOHs_minSeg100.bed'\n",
    "\n",
    "def split_loh_regions():\n",
    "\n",
    "    out_path = '/home/local/work/data/hgsvc/roi/loh_splits'\n",
    "\n",
    "    df = pd.read_csv(in_file, sep='\\t', header=0, names=['#chrom', 'start', 'end', 'sample'])\n",
    "\n",
    "    for sample, regions in df.groupby('sample'):\n",
    "        sub = regions.sort_values(['#chrom', 'start'], inplace=False)\n",
    "        sub['name'] = sub['sample'] + '_' + sub['#chrom'] + '_' + sub['start'].astype(str)\n",
    "        out_file = '{}_loh.bed'.format(sample)\n",
    "        sub.to_csv(\n",
    "            os.path.join(out_path, out_file),\n",
    "            sep='\\t',\n",
    "            columns=['#chrom', 'start', 'end', 'name'],\n",
    "            header=True,\n",
    "            index=False)\n",
    "    return\n",
    "\n",
    "# split_loh_regions()\n",
    "\n",
    "def load_segdup_overlap():\n",
    "    \"\"\"\n",
    "     bedtools intersect -wao\n",
    "     -a 20201026_DP_LOHs_minSeg100.bed\n",
    "     -b GRCh38_segdups.bed\n",
    "     > 20201026_DP_LOHs_minSeg100.sd-ovl.bed \n",
    "    \"\"\"\n",
    "    path = '/home/local/work/data/hgsvc/roi/20201026_DP_LOHs_minSeg100.sd-ovl.bed'\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        sep='\\t',\n",
    "        header=None,\n",
    "        names=[\n",
    "            'chrom',\n",
    "            'start',\n",
    "            'end',\n",
    "            'sample',\n",
    "            'sd_chrom',\n",
    "            'sd_start',\n",
    "            'sd_end',\n",
    "            'sd_name',\n",
    "            'sd_pct_id',\n",
    "            'sd_strand',\n",
    "            'overlap'\n",
    "        ],\n",
    "        usecols=['chrom', 'start', 'end', 'sample', 'sd_pct_id', 'overlap']\n",
    "    )\n",
    "    df['region_id'] = df['sample'] + '_' + df['chrom'] + '_' + df['start'].astype(str)\n",
    "    \n",
    "    uniq_values = []\n",
    "    uniq_index = []\n",
    "    \n",
    "    for idx, row in df.groupby('region_id')[['sd_pct_id', 'overlap']]:\n",
    "        region_id = row['region_id'].values[0]\n",
    "        chrom = row['chrom'].values[0]\n",
    "        start = row['start'].values[0]\n",
    "        end = row['end'].values[0]\n",
    "        \n",
    "        if row['overlap'].values[0] == 0:\n",
    "            uniq_values.append((chrom, start, end, 0., 0.))\n",
    "            uniq_index.append(region_id)\n",
    "            continue\n",
    "        total_overlap = row['overlap'].sum()\n",
    "        region_size = (row['end'] - row['start']).values[0]\n",
    "        sd_weights = row['overlap'] / total_overlap\n",
    "        avg_sd_pct = (np.average(row['sd_pct_id'], weights=sd_weights) /  1000).round(3)\n",
    "        avg_sd_ovl = (total_overlap / region_size).round(3)\n",
    "        \n",
    "        uniq_values.append((chrom, start, end, avg_sd_pct, avg_sd_ovl))\n",
    "        uniq_index.append(region_id)\n",
    "        \n",
    "    df = pd.DataFrame(\n",
    "        uniq_values,\n",
    "        columns=['chrom', 'start', 'end', 'avg_sd_pct_id', 'avg_sd_pct_ovl'],\n",
    "        index=uniq_index\n",
    "    )\n",
    "    df.index.name = 'region_id'\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "cov_tables = '/home/local/work/data/hgsvc/roi/loh_splits/avg_cov'\n",
    "\n",
    "table_columns = [\n",
    "    'region_id',\n",
    "    'region_size',\n",
    "    'coverage_bp',\n",
    "    'coverage_cum',\n",
    "    'coverage_mean',\n",
    "    'coverage_nz_mean'\n",
    "]\n",
    "\n",
    "def load_cov_tables():\n",
    "    merged = col.defaultdict(list)\n",
    "    tables = [f for f in os.listdir(cov_tables) if f.endswith('.tsv')]\n",
    "    for table in tables:\n",
    "        sample = table.split('_')[0]\n",
    "        if '-clr' in table:\n",
    "            tech = 'CLR'\n",
    "        else:\n",
    "            tech = 'HiFi'\n",
    "        hap = table.rstrip('cov_loh.tsv').rsplit('.', 1)[-1]\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(cov_tables, table),\n",
    "            sep='\\t',\n",
    "            header=None,\n",
    "            names=table_columns\n",
    "        )\n",
    "        df['sample'] = sample\n",
    "        df['tech'] = tech\n",
    "        df['coverage_pct'] = (df['coverage_bp'] / df['region_size']).round(2)\n",
    "        df.columns = [hap + '_' + c if c.startswith('coverage') else c for c in df.columns]\n",
    "        df.index = pd.MultiIndex.from_tuples(\n",
    "            [(sample, tech, reg_id) for reg_id in df['region_id']],\n",
    "            names=['sample', 'tech', 'region_id']\n",
    "        )\n",
    "        df.drop(\n",
    "            [\n",
    "                'region_size',\n",
    "                'region_id',\n",
    "                'sample',\n",
    "                'tech',\n",
    "\n",
    "            ], axis=1, inplace=True\n",
    "        )\n",
    "        merged[(sample, tech)].append(df)\n",
    "        \n",
    "    final_merge = []\n",
    "    for k, v in merged.items():\n",
    "        tmp = v[0].join(v[1:], how='outer')\n",
    "        tmp.drop(\n",
    "            [\n",
    "                'h1_coverage_cum',\n",
    "                'h2_coverage_cum',\n",
    "                'un_coverage_cum'\n",
    "            ], axis=1, inplace=True\n",
    "        )\n",
    "        final_merge.append(tmp)\n",
    "    final_merge = pd.concat(final_merge, axis=0, ignore_index=False)\n",
    "    \n",
    "    return final_merge\n",
    "\n",
    "\n",
    "def load_input_coverage():\n",
    "    path = '/home/local/work/data/hgsvc/fig1_panels/input_read_stats/input_read_stats.tsv'\n",
    "    df = pd.read_csv(path, sep='\\t', header=None, names=['sample', 'super_pop', 'pop', 'tech', 'input_cov'])\n",
    "    df.index = pd.MultiIndex.from_tuples(\n",
    "        [(row[0], row[3]) for row in df.itertuples(index=False)],\n",
    "        names=['sample', 'tech']\n",
    "    )\n",
    "    df = df.loc[~df['sample'].isin(['HG00733', 'HG00514', 'NA19240']), :].copy()\n",
    "    df.drop(['sample', 'tech', 'pop'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = load_cov_tables()\n",
    "input_cov = load_input_coverage()\n",
    "sd_ovl = load_segdup_overlap()\n",
    "\n",
    "df = df.join(sd_ovl, how='outer')\n",
    "df = df.join(input_cov, how='outer')\n",
    "\n",
    "df['read_cov'] = (df['h1_coverage_mean'] + df['h2_coverage_mean'] + df['un_coverage_mean']).round(3)\n",
    "\n",
    "df['feat_un_cov'] = (df['un_coverage_mean'] / df['input_cov']).round(3)\n",
    "df['feat_h1_cov'] = (df['h1_coverage_mean'] / df['input_cov']).round(3)\n",
    "df['feat_h2_cov'] = (df['h2_coverage_mean'] / df['input_cov']).round(3)\n",
    "df['feat_locus_cov'] = (df['read_cov'] / df['input_cov']).round(3)\n",
    "df['feat_h1_loc'] = df['h1_coverage_pct']\n",
    "df['feat_h2_loc'] = df['h2_coverage_pct']\n",
    "df['feat_un_loc'] = df['un_coverage_pct']\n",
    "df['feat_avg_sd_id'] = df['avg_sd_pct_id']\n",
    "df['feat_avg_sd_ovl'] = df['avg_sd_pct_ovl']\n",
    "\n",
    "cluster_features = [\n",
    "    c for c in df.columns if c.startswith('feat')\n",
    "]\n",
    "\n",
    "df['cluster_LOH'] = -1\n",
    "df['cluster_LOHSD'] = -1\n",
    "\n",
    "model_output = '/home/local/work/data/hgsvc/roi'\n",
    "if False:\n",
    "\n",
    "    clustering_models = []\n",
    "\n",
    "    for num_clust in [2, 3]:\n",
    "        \n",
    "        if num_clust == 2:\n",
    "            use_features = [c for c in cluster_features if '_sd_' not in c]\n",
    "        else:\n",
    "            use_features = cluster_features\n",
    "\n",
    "        kmeans_model = sklclust.KMeans(\n",
    "            n_clusters=num_clust,\n",
    "            init='k-means++',\n",
    "            n_init=50,\n",
    "            max_iter=500,\n",
    "            tol=0.0001,\n",
    "            verbose=False,\n",
    "            algorithm='full'\n",
    "        )\n",
    "\n",
    "        labels = kmeans_model.fit_predict(df[use_features])\n",
    "        label = 'cluster_LOH' if num_clust == 2 else 'cluster_LOHSD'\n",
    "        \n",
    "        for l in set(labels):\n",
    "            df.loc[labels == l, label] = l\n",
    "            \n",
    "        \n",
    "        model_file = 'kmeans_n{}.pck'.format(num_clust)\n",
    "        with open(os.path.join(model_output, model_file), 'wb') as dump:\n",
    "            _ = pck.dump(kmeans_model, dump)\n",
    "            \n",
    "    with pd.HDFStore(os.path.join(model_output, 'loh_regions_labeled.h5'), 'w') as hdf:\n",
    "        hdf.put('loh_regions', df)\n",
    "\n",
    "with pd.HDFStore(os.path.join(model_output, 'loh_regions_labeled.h5'), 'r') as hdf:\n",
    "        df = hdf['loh_regions']\n",
    "\n",
    "df['cLOH_is_LOH'] = -1\n",
    "df['cLOHSD_is_LOH'] = -1\n",
    "\n",
    "df.loc[df['cluster_LOH'] == 0, 'cLOH_is_LOH'] = 0\n",
    "df.loc[df['cluster_LOH'] == 1, 'cLOH_is_LOH'] = 1\n",
    "\n",
    "df.loc[df['cluster_LOHSD'] == 0, 'cLOHSD_is_LOH'] = 1\n",
    "df.loc[df['cluster_LOHSD'] == 1, 'cLOHSD_is_LOH'] = 0\n",
    "df.loc[df['cluster_LOHSD'] == 2, 'cLOHSD_is_LOH'] = -1\n",
    "\n",
    "df['BNG_SV_DEL_clusterID'] = 'none'\n",
    "df['BNG_SV_DEL_cluster_size'] = 'none'\n",
    "\n",
    "sv_cluster_file = '/home/local/work/data/hgsvc/roi/bng_sv_del_nonovl.tsv'\n",
    "sv_clusters = pd.read_csv(sv_cluster_file, sep='\\t', header=0)\n",
    "\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    chrom_match = row['chrom'].strip('chr') + '$'\n",
    "    select_chrom = sv_clusters['Chr'].str.match(chrom_match)\n",
    "    select_start = (sv_clusters['Start'] - 100000) < row['end']\n",
    "    select_end = (sv_clusters['End'] + 100000) > row['start']\n",
    "    select_pop = sv_clusters[row['super_pop']] > 0\n",
    "    select_combined = select_chrom & select_start & select_end & select_pop\n",
    "    if not select_combined.any():\n",
    "        continue\n",
    "    selected = sv_clusters.loc[select_combined, :]\n",
    "    labels = ','.join(selected['#ClusterID'].astype(str).values)\n",
    "    sizes = ','.join(selected['ClusterSVsize'].astype(str).values)\n",
    "    df.loc[idx, 'BNG_SV_DEL_clusterID'] = labels\n",
    "    df.loc[idx, 'BNG_SV_DEL_cluster_size'] = sizes\n",
    "    \n",
    "df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "total = df.shape[0]\n",
    "loh_loh = (df['cLOH_is_LOH'] == 1).sum()\n",
    "lohsd_loh = (df['cLOH_is_LOH'] == 1).sum()\n",
    "\n",
    "\n",
    "loh_regions = set()\n",
    "loh_regions_with_del = set()\n",
    "loh_counts = col.Counter()\n",
    "loh_regions_bng_cluster = set()\n",
    "\n",
    "clr_lengths = set()\n",
    "hifi_lengths = set()\n",
    "\n",
    "for (sample, tech), regions in df.groupby(['sample', 'tech']):\n",
    "    \n",
    "    select_is_loh = regions['cLOH_is_LOH'] == 1\n",
    "    select_has_cluster = regions['BNG_SV_DEL_clusterID'] != 'none'\n",
    "    \n",
    "    sample_loh = regions.loc[select_is_loh, 'region_id'].values\n",
    "    \n",
    "    region_lengths = (regions.loc[select_is_loh, 'end'] - regions.loc[select_is_loh, 'start']).values\n",
    "    if tech == 'CLR':\n",
    "        clr_lengths = clr_lengths.union(\n",
    "            set(\n",
    "                [(region_id, region_length) \n",
    "                 for region_id, region_length in zip(sample_loh, region_lengths)\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        hifi_lengths = hifi_lengths.union(\n",
    "            set(\n",
    "                [(region_id, region_length) \n",
    "                 for region_id, region_length in zip(sample_loh, region_lengths)\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    loh_counts[(sample, tech)] += sample_loh.size\n",
    "    loh_regions = loh_regions.union(set(sample_loh))\n",
    "    \n",
    "    sample_loh_with_del = regions.loc[select_is_loh & select_has_cluster, 'region_id'].values\n",
    "    loh_regions_with_del = loh_regions_with_del.union(set(sample_loh_with_del))\n",
    "    \n",
    "\n",
    "print('clr mean ', np.mean(np.array([l for i, l in clr_lengths], dtype=np.int64)))\n",
    "print('hifi mean ', np.mean(np.array([l for i, l in hifi_lengths], dtype=np.int64)))\n",
    "    \n",
    "print(loh_counts)\n",
    "print('Unique LOH regions ', len(loh_regions))\n",
    "print('Unique LOH regions with DEL ', len(loh_regions_with_del))\n",
    "print('Regions identified ', df['region_id'].nunique())\n",
    "\n",
    "\n",
    "hifi_loh = [c for (s, t), c in loh_counts.items() if t == 'HiFi']\n",
    "clr_loh = [c for (s, t), c in loh_counts.items() if t == 'CLR']\n",
    "\n",
    "print(np.median(hifi_loh))\n",
    "print(np.median(clr_loh))\n",
    "raise\n",
    "    \n",
    "    \n",
    "\n",
    "df.sort_values(['sample', 'tech', 'chrom', 'start'], inplace=True)\n",
    "\n",
    "dump_columns = [\n",
    "    'chrom',\n",
    "    'start',\n",
    "    'end',\n",
    "    'sample',\n",
    "    'tech',\n",
    "    'cLOH_is_LOH',\n",
    "    'cLOHSD_is_LOH',\n",
    "    'BNG_SV_DEL_clusterID',\n",
    "    'BNG_SV_DEL_cluster_size'\n",
    "]\n",
    "dump_columns.extend(cluster_features)\n",
    "\n",
    "out_dump = '/home/local/work/data/hgsvc/roi/20201026_DP_LOHs_minSeg100.BNG-DEL.bed'\n",
    "\n",
    "with open(out_dump, 'w') as dump:\n",
    "    _ = dump.write('#')\n",
    "    df.to_csv(\n",
    "        dump,\n",
    "        sep='\\t',\n",
    "        header=True,\n",
    "        index=False,\n",
    "        columns=dump_columns\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
